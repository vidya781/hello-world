In a business setting, most data may not be stord in text or Excel files.SQL-based relational databases(such as SQL server, PostGresSQL and MySQL)
are in wide use and many alternative databases have become quite popular.The choice of Database is normally dependent on the performance,
data integrity and scalability needs of an application.

Here we take the example of loading dat from SQL into a dataframe and we use pandas to simplify the process.

(It is advised to use Jupyter Notebook for this purpose)

import sqlite3

query = """
CREATE TABLE test
(a VARCHAR(20),b VARCHAR(20),
 c REAL, d INTEGER
 );"""
 
 con = sqlite3.connect('mydata.sqlite')
 
 con.execute(query)
 con.commit()
 
 data = [('Atlanta','Georgia',1.25,6),
         ('Tallahassee','Florida',2.6,3),
         ('Sacramento','California',1.7,5)]
         
 stmt = 'INSERT INTO test VALUES(?,?,?,?)"
 
 con.executemany(stmt,data)
 
 con.commit()
 cursor = con.execute('select *from test')
 rows = cursor.fetchall()
 rows
 
 cursor.description   # pass a list of tuples to the DatFrame constructor but also need the column names contained in the cursor's description attribute
 
 pd.DataFrame(rows,columns=[x[0] for x in cursor.description])
 
 In order to avoid repetitive munging, one could use the SQLAlchemy which is a popular Python SQL toolkit that abstracts away many of the common differences
 between SQL databases. Pandas has a read_sql function that enables one to read data easily from a general SQLAlchemy connection.
 
 import sqlalchemy as sqla
 db = sqla.create_engine('sqlite:///mydata.sqlite')
 
 pd.read_sql('select * from test',db)
 
